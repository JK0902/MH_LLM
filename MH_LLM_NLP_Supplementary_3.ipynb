{
  "cells": [
    {
      "cell_type": "code",
      "id": "zqafDwVaFsZRpg5PZjZaWuRC",
      "metadata": {
        "tags": [],
        "id": "zqafDwVaFsZRpg5PZjZaWuRC"
      },
      "source": [
        "%%capture\n",
        "!pip install bertopic\n",
        "#!pip install cohere\n",
        "!pip install altair\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import requests\n",
        "import re\n",
        "from tabulate import tabulate\n",
        "from matplotlib.pyplot import figure\n",
        "import seaborn.objects as so\n",
        "import scipy.stats\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from bertopic import BERTopic\n",
        "from bertopic.vectorizers import ClassTfidfTransformer\n",
        "#import cohere\n",
        "from bertopic.representation import Cohere\n",
        "from bertopic.backend import CohereBackend\n",
        "import umap\n",
        "import altair as alt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.cluster import hierarchy\n",
        "\n",
        "import bigframes.pandas as bpd\n",
        "import tensorflow_hub\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from transformers.pipelines import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import colorcet as cc"
      ],
      "metadata": {
        "id": "pBs_MKxfDPMA"
      },
      "id": "pBs_MKxfDPMA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "# Set up the Google Cloud Storage client\n",
        "client = storage.Client()\n",
        "\n",
        "# Specify your bucket and file path\n",
        "bucket = client.bucket(\"\")  # Replace with your bucket name\n",
        "blob = bucket.blob(\"\")  # Replace with your file path\n",
        "\n",
        "# Read the file directly into a pandas DataFrame without saving locally\n",
        "content = blob.download_as_text()\n",
        "com_id_df = pd.read_csv(io.StringIO(content))\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "com_id_df.head()"
      ],
      "metadata": {
        "id": "36eFNDMbDjg8"
      },
      "id": "36eFNDMbDjg8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install wordcloud matplotlib"
      ],
      "metadata": {
        "id": "OKCTWM5aD62_"
      },
      "id": "OKCTWM5aD62_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "def download_nltk_resources():\n",
        "    \"\"\"\n",
        "    Ensure that necessary NLTK resources are available.\n",
        "    Download them only if they are not already downloaded.\n",
        "    \"\"\"\n",
        "    resources = {\n",
        "        'tokenizers/punkt': 'punkt',\n",
        "        'corpora/wordnet': 'wordnet',\n",
        "        'taggers/averaged_perceptron_tagger': 'averaged_perceptron_tagger'\n",
        "    }\n",
        "\n",
        "    for path, package in resources.items():\n",
        "        try:\n",
        "            nltk.data.find(path)\n",
        "        except LookupError:\n",
        "            nltk.download(package)\n",
        "\n",
        "# Call the function to check and download resources\n",
        "download_nltk_resources()"
      ],
      "metadata": {
        "id": "G0qDl-rTD_E4"
      },
      "id": "G0qDl-rTD_E4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_special_patterns(text):\n",
        "    # This function can be adjusted if specific unwanted patterns are observed\n",
        "    text = re.sub(r'\\{[^ ]* ', ' ', text)\n",
        "    text = re.sub(r'\\\\[^ ]* ', ' ', text)\n",
        "    text = re.sub(r'Arial;}}[^ ]* ', ' ', text)\n",
        "    text = re.sub(r';[^ ]* ', ' ', text)\n",
        "    text = re.sub(r',\\\\[^ ]* ', ' ', text)\n",
        "    return text\n",
        "\n",
        "def clean_and_remove_rtf(text):\n",
        "    # Remove RTF control words, formatting codes, and unnecessary curly braces content\n",
        "    text = re.sub(r'\\\\[a-zA-Z]+\\d* ?', '', text)  # Removes control words with optional numbers\n",
        "    text = re.sub(r'\\\\[a-zA-Z]+', '', text)  # Cleans any remaining control words\n",
        "    text = re.sub(r'{[^{}]*}', '', text)  # Aggressively remove content within curly braces\n",
        "    text = re.sub(r'\\btsWidth\\d*\\b', '', text)  # Specific removal of 'tsWidth' followed by any numbers\n",
        "    text = re.sub(r'\\bcl[a-zA-Z0-9]+\\b', '', text)  # Removes words starting with 'cl' that are part of cell definitions\n",
        "    text = re.sub(r'row[a-zA-Z0-9]+\\b', '', text)  # Remove patterns starting with 'row' typical in table definitions\n",
        "    text = re.sub(r'\\brd[a-zA-Z0-9]+', '', text)  # Remove 'rd' prefixed RTF controls like 'rdrnone'\n",
        "    text = re.sub(r'\\b[a-zA-Z0-9]{1,3}\\b', '', text)  # Remove isolated short words\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  # Remove non-ASCII characters\n",
        "    text = re.sub(r'\\s{2,}', ' ', text).strip()  # Normalize whitespace\n",
        "    text = re.sub(r'\\bArial\\b', '', text, flags=re.IGNORECASE)  # Remove the name 'Arial' completely\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    \"\"\"Map NLTK POS tag to a format recognized by WordNetLemmatizer\"\"\"\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag[0].upper(), wordnet.NOUN)\n",
        "\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "\n",
        "def process_msg_txt(msg):\n",
        "    \"\"\"\n",
        "    Process message texts such that they are standardized.\n",
        "    \"\"\"\n",
        "    if isinstance(msg, str):\n",
        "        msg = remove_special_patterns(msg)\n",
        "        msg = clean_and_remove_rtf(msg)\n",
        "        return lemmatize_text(msg) if msg else \"\"\n",
        "    return \"\""
      ],
      "metadata": {
        "id": "VL8WZH_HE7pF"
      },
      "id": "VL8WZH_HE7pF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_msg_txt(row):\n",
        "    \"\"\"\n",
        "    Assign messages that meet criteria for a relevant message a value of 1,\n",
        "    otherwise assign value of 0.\n",
        "\n",
        "    Criteria:\n",
        "      - msg is > 50 characters\n",
        "      - subject isn't standard message for survey for recent video visit\n",
        "    \"\"\"\n",
        "    if len(row[\"msg_txt_processed\"]) <= 50:\n",
        "        return 0\n",
        "    elif row[\"msg_txt_processed\"].startswith(\"We were unable to reach you by phone.\"):\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "def process_filter_msg_df(msg_df):\n",
        "    \"\"\"\n",
        "    In: msg_df in the form of RQ_BC_from_pat_post_to_onc0000000000{0:0=2d}.csv\n",
        "    Out: msg_df that has been processed and filtered\n",
        "    \"\"\"\n",
        "    # Determine the correct column name for the text data\n",
        "    if \"msg_txt\" in msg_df.columns:\n",
        "        text_column = \"msg_txt\"\n",
        "    elif \"rtf_txt\" in msg_df.columns:\n",
        "        text_column = \"rtf_txt\"\n",
        "    else:\n",
        "        raise ValueError(\"Neither 'msg_txt' nor 'rtf_txt' columns found in DataFrame.\")\n",
        "\n",
        "    # Process the message text\n",
        "    msg_df[\"msg_txt_processed\"] = msg_df[text_column].apply(process_msg_txt)\n",
        "    msg_df[\"msg_txt_flag\"] = msg_df.apply(filter_msg_txt, axis=1)\n",
        "\n",
        "    # Filter and remove duplicates\n",
        "    msg_processed_filtered_df = msg_df[msg_df[\"msg_txt_flag\"] == 1]\n",
        "    msg_processed_filtered_df = msg_processed_filtered_df.drop_duplicates(subset=[\"msg_txt_processed\"], keep=\"first\")\n",
        "\n",
        "    return msg_processed_filtered_df\n",
        "\n",
        "def filter_by_id(msg_df, id_arr):\n",
        "    \"\"\"\n",
        "    In: msg_df and array of IDs to filter by (i.e., ID arr of DM patients)\n",
        "    Out: msg_df with only relevant patients\n",
        "    \"\"\"\n",
        "    return msg_df.loc[msg_df[\"anon_id\"].isin(id_arr)]\n"
      ],
      "metadata": {
        "id": "eIEaGD1QW62W"
      },
      "id": "eIEaGD1QW62W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_diagnosis(row):\n",
        "    \"\"\"\n",
        "    Helper function to change ICD-10 codes into diagnoses.\n",
        "    \"\"\"\n",
        "    if row[\"icd10\"].startswith(\"E08\") or row[\"icd10\"].startswith(\"E09\"):\n",
        "        return \"DM\"\n",
        "    elif row[\"icd10\"].startswith(\"F32\"):\n",
        "        return \"depression\"\n",
        "    else:\n",
        "        print(row[\"icd10\"])\n",
        "        raise Exception(\"Invalid ICD-10 code\")\n",
        "\n",
        "\n",
        "def get_diagnostic_grouping(diagnoses):\n",
        "    \"\"\"\n",
        "    Helper function to change diagnoses into diagnostic groupings.\n",
        "    \"\"\"\n",
        "    if \"DM\" in diagnoses and \"depression\" in diagnoses:\n",
        "        return \"co-morbid\"\n",
        "    elif \"DM\" in diagnoses:\n",
        "        return \"DM\"\n",
        "    elif \"depression\" in diagnoses:\n",
        "        return \"depression\"\n",
        "    else:\n",
        "        raise Exception(\"Unexpected diagnosis grouping\")\n"
      ],
      "metadata": {
        "id": "SWPWpNqXFNCD"
      },
      "id": "SWPWpNqXFNCD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert ICD codes into diagnoses\n",
        "com_id_df[\"diagnosis\"] = com_id_df.apply(lambda row: get_diagnosis(row), axis=1)\n",
        "\n",
        "# Group by patient ID and aggregate diagnoses into lists\n",
        "diagnosis_groups = com_id_df.groupby(\"anon_id\")[\"diagnosis\"].apply(list).reset_index()\n",
        "\n",
        "# Convert aggregated diagnoses into diagnostic groupings\n",
        "diagnosis_groups[\"Diagnostic Group\"] = diagnosis_groups[\"diagnosis\"].apply(get_diagnostic_grouping)\n",
        "\n",
        "# Filter to keep only those with \"co-morbid\" diagnosis\n",
        "co_morbid_df = diagnosis_groups[diagnosis_groups[\"Diagnostic Group\"] == \"co-morbid\"]\n",
        "\n",
        "# Merge the diagnostic grouping back to the original DataFrame\n",
        "com_id_df = com_id_df.merge(diagnosis_groups[[\"anon_id\", \"Diagnostic Group\"]], on=\"anon_id\", how=\"left\")\n",
        "\n",
        "# Get valid IDs for \"co-morbid\" group only\n",
        "final_valid_ids = co_morbid_df[\"anon_id\"].values\n",
        "\n",
        "# If you want to see the result\n",
        "print(co_morbid_df.shape)\n",
        "print(final_valid_ids)\n",
        "\n"
      ],
      "metadata": {
        "id": "11JklcIHjwD2"
      },
      "id": "11JklcIHjwD2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run pipeline for all 7 subsets of the dataset\n",
        "for num in range(0, 7):\n",
        "\n",
        "    # Generate the correct file name\n",
        "    file_name = \"Your_file_name_0000000000{0:0=2d}.csv\".format(num)\n",
        "\n",
        "    # Access the file in Google Cloud Storage\n",
        "    blob = bucket.blob(file_name)\n",
        "\n",
        "    # Download the content as a string\n",
        "    content = blob.download_as_text()\n",
        "\n",
        "    # Read the string content into a pandas DataFrame\n",
        "    temp_msg_all_df = pd.read_csv(io.StringIO(content))\n",
        "\n",
        "    # Process and filter the DataFrame\n",
        "    temp_msg_all_df = process_filter_msg_df(temp_msg_all_df)\n",
        "    temp_msg_com_df = filter_by_id(temp_msg_all_df, final_valid_ids)\n",
        "\n",
        "    # Concatenate the DataFrames\n",
        "    if num == 0:\n",
        "        msg_com_all_df = temp_msg_com_df\n",
        "    else:\n",
        "        msg_com_all_df = pd.concat([msg_com_all_df, temp_msg_com_df])\n",
        "\n",
        "    # Drop any new duplicates that have arisen across data subsets\n",
        "    msg_com_all_df = msg_com_all_df.drop_duplicates(subset=[\"msg_txt_processed\"], keep=\"first\")\n",
        "\n",
        "    # Print the shape of the DataFrame after processing each subset\n",
        "    print(msg_com_all_df.shape)\n"
      ],
      "metadata": {
        "id": "Fq3rgd6HS98R"
      },
      "id": "Fq3rgd6HS98R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(msg_com_all_df.shape)\n",
        "print(msg_com_all_df.columns.values)"
      ],
      "metadata": {
        "id": "3KT7Lb83hhyp"
      },
      "id": "3KT7Lb83hhyp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add ICD-10 diagnosis and diagnostic group to the dataframe of messages\n",
        "msg_com_all_df = msg_com_all_df.merge(com_id_df[[\"anon_id\", \"diagnosis\", \"Diagnostic Group\"]], left_on=\"anon_id\", right_on=\"anon_id\")\n",
        "\n",
        "# Check value counts for diagnosis\n",
        "print(msg_com_all_df[\"diagnosis\"].value_counts())\n"
      ],
      "metadata": {
        "id": "zxAT7tsOh_I3"
      },
      "id": "zxAT7tsOh_I3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create column to flag columns that meet criteria\n",
        "msg_com_all_df[\"msg_txt_processed\"] = msg_com_all_df.apply(lambda row: process_msg_txt(row[\"msg_txt\"]), axis=1)\n",
        "msg_com_all_df[\"msg_txt_flag\"] = msg_com_all_df.apply(lambda row: filter_msg_txt(row), axis=1)\n",
        "\n",
        "# Create dataframe containing only messages that meet processed criteria\n",
        "msg_processed_df = msg_com_all_df[msg_com_all_df[\"msg_txt_flag\"] == 1]\n",
        "\n",
        "# Drop rows with duplicate messages\n",
        "msg_processed_df = msg_processed_df.drop_duplicates(subset=[\"msg_txt_processed\"], keep=\"first\")"
      ],
      "metadata": {
        "id": "vtzxCTUH-1Fc"
      },
      "id": "vtzxCTUH-1Fc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msg_com_all_df.shape"
      ],
      "metadata": {
        "id": "CbmD9lfR-vd-"
      },
      "id": "CbmD9lfR-vd-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1st Topic Model**"
      ],
      "metadata": {
        "id": "lBzQYe2ToQgG"
      },
      "id": "lBzQYe2ToQgG"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create list of messages for formatting into model\n",
        "short_docs = msg_processed_df[\"msg_txt_processed\"].values.tolist()"
      ],
      "metadata": {
        "id": "aHBcF3s9ZeeO"
      },
      "id": "aHBcF3s9ZeeO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Pre-calculate embeddings\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = embedding_model.encode(short_docs, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "CZrMQrVbZi_f"
      },
      "id": "CZrMQrVbZi_f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import Birch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "\n",
        "# Define your stop words and zero-shot topic list\n",
        "zeroshot_common_topic_list = [\"depression\"]\n",
        "stop_words_list = ['dr', 'doctor', 'june', 'july', 'myhealth']\n",
        "\n",
        "vectorizer_model = CountVectorizer(stop_words=stop_words_list)\n",
        "\n",
        "# Define your BERTopic model\n",
        "topic_model = BERTopic(\n",
        "    embedding_model=embedding_model,\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    min_topic_size=15,\n",
        "    zeroshot_topic_list=zeroshot_common_topic_list,\n",
        "    zeroshot_min_similarity=.82,\n",
        "    representation_model=KeyBERTInspired()\n",
        ")\n",
        "\n",
        "# Fit your BERTopic model and transform documents to get topics and embeddings\n",
        "topics, _ = topic_model.fit_transform(short_docs, embeddings)"
      ],
      "metadata": {
        "id": "9ipWTN7UZzeF"
      },
      "id": "9ipWTN7UZzeF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get topic summary\n",
        "topic_info_df = topic_model.get_topic_info()"
      ],
      "metadata": {
        "id": "nFv_r0hzZ3ni"
      },
      "id": "nFv_r0hzZ3ni",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View topic summary\n",
        "topic_info_df.head(40)"
      ],
      "metadata": {
        "id": "m8bWV7m2Z9Ca"
      },
      "id": "m8bWV7m2Z9Ca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert numpy array to pandas DataFrame\n",
        "df_topic_info_df = pd.DataFrame(topic_info_df)\n",
        "\n",
        "# Now save the DataFrame to a CSV file\n",
        "csv_data = df_topic_info_df.to_csv(index=False)"
      ],
      "metadata": {
        "id": "bUdfXM0Ca4FJ"
      },
      "id": "bUdfXM0Ca4FJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the file path and name in the bucket\n",
        "file_name = \"\"\n",
        "\n",
        "# Create a blob (a file in GCS)\n",
        "blob = bucket.blob(file_name)\n",
        "\n",
        "# Upload the CSV string to GCS\n",
        "blob.upload_from_string(csv_data, content_type='text/csv')\n"
      ],
      "metadata": {
        "id": "awBDo230a7iF"
      },
      "id": "awBDo230a7iF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2nd Topic Model**"
      ],
      "metadata": {
        "id": "C_PM2F4hoH9t"
      },
      "id": "C_PM2F4hoH9t"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import Birch\n",
        "import numpy as np\n",
        "\n",
        "# Load your embedding model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Now, use BIRCH for clustering\n",
        "birch_model = Birch(threshold=0.5, n_clusters=None)  # Adjust threshold and n_clusters as needed\n",
        "birch_model.fit(embeddings)\n",
        "\n",
        "# Predict clusters using BIRCH\n",
        "birch_clusters = birch_model.predict(embeddings)\n",
        "\n",
        "\n",
        "# Create a mapping from BIRCH clusters to documents\n",
        "cluster_to_docs = {}\n",
        "for doc_idx, cluster in enumerate(birch_clusters):\n",
        "    if cluster not in cluster_to_docs:\n",
        "        cluster_to_docs[cluster] = []\n",
        "    cluster_to_docs[cluster].append(doc_idx)\n",
        "\n",
        "# Combine texts by BIRCH cluster\n",
        "df = pd.DataFrame({\n",
        "    'Document': short_docs,\n",
        "    'BIRCH_Cluster': birch_clusters\n",
        "})\n",
        "cluster_texts = df.groupby('BIRCH_Cluster')['Document'].apply(lambda docs: ' '.join(docs)).reset_index()\n",
        "\n",
        "# Create a new BERTopic model with custom topics\n",
        "new_topic_model = BERTopic(\n",
        "    embedding_model=embedding_model,\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    min_topic_size=15,\n",
        "    zeroshot_topic_list=zeroshot_common_topic_list,\n",
        "    zeroshot_min_similarity=.82,\n",
        "    representation_model=KeyBERTInspired()\n",
        ")\n",
        "\n",
        "# Fit the new BERTopic model on the combined cluster texts\n",
        "new_topics, _ = new_topic_model.fit_transform(cluster_texts['Document'].tolist())\n",
        "\n",
        "# Get topic summaries for each cluster\n",
        "topic_info = new_topic_model.get_topic_info()"
      ],
      "metadata": {
        "id": "Euo0swK2bbsj"
      },
      "id": "Euo0swK2bbsj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View topic summary\n",
        "topic_info.head(40)"
      ],
      "metadata": {
        "id": "9EpRMpYxbgSC"
      },
      "id": "9EpRMpYxbgSC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert numpy array to pandas DataFrame\n",
        "df_topic_info = pd.DataFrame(topic_info)\n",
        "\n",
        "# Now save the DataFrame to a CSV file\n",
        "csv_data = df_topic_info.to_csv(index=False)"
      ],
      "metadata": {
        "id": "ThgrrrFhbklk"
      },
      "id": "ThgrrrFhbklk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the file path and name in the bucket\n",
        "file_name = \"\"\n",
        "\n",
        "# Create a blob (a file in GCS)\n",
        "blob = bucket.blob(file_name)\n",
        "\n",
        "# Upload the CSV string to GCS\n",
        "blob.upload_from_string(csv_data, content_type='text/csv')"
      ],
      "metadata": {
        "id": "QBnkqfg6bn2x"
      },
      "id": "QBnkqfg6bn2x",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "MH_LLM_NLP_Supplementary_3.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
