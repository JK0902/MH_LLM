{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkIF-qKfOvFl"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XZf_4VEOvFo"
      },
      "source": [
        "## Getting Started (Gemini Pro 1.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################### The code was written for the analysis in Vertex AI by Google LLC ####################"
      ],
      "metadata": {
        "id": "pfRlyLquRpef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE20na1OOvFo"
      },
      "source": [
        "### Install Vertex AI SDK for Gen AI Evaluation Service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abLuRgBzOvFp"
      },
      "outputs": [],
      "source": [
        "%pip install -U -q google-cloud-aiplatform[evaluation]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYJVHBVSZgTX"
      },
      "source": [
        "### Install other required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "squkG3h9ZZs8"
      },
      "outputs": [],
      "source": [
        "%pip install -U -q datasets\n",
        "%pip install -U -q anthropic[vertex]\n",
        "%pip install -U -q openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe2lLnYuOvFp"
      },
      "source": [
        "### Restart runtime\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
        "\n",
        "The restart might take a minute or longer. After it's restarted, continue to the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3FDZs3qOvFp"
      },
      "outputs": [],
      "source": [
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqDc-oyiOvFp"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1oLkh17OvFp"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ygOCeYoOvFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "595b51b6-67b1-4938-df73-a4d18f8e9d22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: google.colab.auth.authenticate_user() is not supported in Colab Enterprise.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wyNclIAOvFp"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTL_YzF9OvFq"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    raise ValueError(\"Please set your PROJECT_ID\")\n",
        "\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBQgjn5wOvFq"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TmyCxUSOvFq"
      },
      "outputs": [],
      "source": [
        "from anthropic import AnthropicVertex\n",
        "from google.auth import default, transport\n",
        "import openai\n",
        "from vertexai.evaluation import (\n",
        "    EvalTask,\n",
        "    MetricPromptTemplateExamples,\n",
        "    PairwiseMetric,\n",
        "    PointwiseMetric,\n",
        "    PointwiseMetricPromptTemplate,\n",
        ")\n",
        "from vertexai.generative_models import GenerativeModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfQ7sPtOjZOw"
      },
      "source": [
        "### Library settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjWUgU1TjZOw"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "import logging\n",
        "import warnings\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.ERROR)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_Gw6YLeOvFq"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8imb3UdOvFq"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "import random\n",
        "import string\n",
        "\n",
        "from IPython.display import HTML, Markdown, display\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "def display_explanations(eval_result, metrics=None, n=1):\n",
        "    \"\"\"Display the explanations.\"\"\"\n",
        "    style = \"white-space: pre-wrap; width: 1500px; overflow-x: auto;\"\n",
        "    metrics_table = eval_result.metrics_table\n",
        "    df = metrics_table.sample(n=n)\n",
        "\n",
        "    if metrics:\n",
        "        df = df.filter(\n",
        "            [\"response\", \"baseline_model_response\"]\n",
        "            + [\n",
        "                metric\n",
        "                for metric in df.columns\n",
        "                if any(selected_metric in metric for selected_metric in metrics)\n",
        "            ]\n",
        "        )\n",
        "    for index, row in df.iterrows():\n",
        "        for col in df.columns:\n",
        "            display(HTML(f\"<div style='{style}'><h4>{col}:</h4>{row[col]}</div>\"))\n",
        "        display(HTML(\"<hr>\"))\n",
        "\n",
        "\n",
        "def display_eval_result(eval_result, title=None, metrics=None):\n",
        "    \"\"\"Display the evaluation results.\"\"\"\n",
        "    summary_metrics, metrics_table = (\n",
        "        eval_result.summary_metrics,\n",
        "        eval_result.metrics_table,\n",
        "    )\n",
        "\n",
        "    metrics_df = pd.DataFrame.from_dict(summary_metrics, orient=\"index\").T\n",
        "    if metrics:\n",
        "        metrics_df = metrics_df.filter(\n",
        "            [\n",
        "                metric\n",
        "                for metric in metrics_df.columns\n",
        "                if any(selected_metric in metric for selected_metric in metrics)\n",
        "            ]\n",
        "        )\n",
        "        metrics_table = metrics_table.filter(\n",
        "            [\n",
        "                metric\n",
        "                for metric in metrics_table.columns\n",
        "                if any(selected_metric in metric for selected_metric in metrics)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    if title:\n",
        "        # Display the title with Markdown for emphasis\n",
        "        display(Markdown(f\"## {title}\"))\n",
        "    # Display the summary metrics DataFrame\n",
        "    display(Markdown(\"### Summary Metrics\"))\n",
        "    display(metrics_df)\n",
        "    # Display the metrics table DataFrame\n",
        "    display(Markdown(\"### Row-based Metrics\"))\n",
        "    display(metrics_table)\n",
        "\n",
        "\n",
        "def display_radar_plot(eval_results, metrics=None):\n",
        "    \"\"\"Plot the radar plot.\"\"\"\n",
        "    fig = go.Figure()\n",
        "    for item in eval_results:\n",
        "        title, eval_result = item\n",
        "        summary_metrics = eval_result.summary_metrics\n",
        "        if metrics:\n",
        "            summary_metrics = {\n",
        "                k.replace(\"/mean\", \"\"): summary_metrics[k]\n",
        "                for k, v in summary_metrics.items()\n",
        "                if any(selected_metric + \"/mean\" in k for selected_metric in metrics)\n",
        "            }\n",
        "        fig.add_trace(\n",
        "            go.Scatterpolar(\n",
        "                r=list(summary_metrics.values()),\n",
        "                theta=list(summary_metrics.keys()),\n",
        "                fill=\"toself\",\n",
        "                name=title,\n",
        "            )\n",
        "        )\n",
        "    fig.update_layout(\n",
        "        polar=dict(radialaxis=dict(visible=True, range=[0, 5])), showlegend=True\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "def display_bar_plot(eval_results_list, metrics=None):\n",
        "    \"\"\"Plot the bar plot.\"\"\"\n",
        "    fig = go.Figure()\n",
        "    data = []\n",
        "\n",
        "    for eval_results in eval_results_list:\n",
        "        title, eval_result = eval_results[0], eval_results[1]\n",
        "\n",
        "        summary_metrics = eval_result.summary_metrics\n",
        "        mean_summary_metrics = [f\"{metric}/mean\" for metric in metrics]\n",
        "        updated_summary_metrics = []\n",
        "        if metrics:\n",
        "            for k, v in summary_metrics.items():\n",
        "                if k in mean_summary_metrics:\n",
        "                    updated_summary_metrics.append((k, v))\n",
        "            summary_metrics = dict(updated_summary_metrics)\n",
        "            # summary_metrics = {k: summary_metrics[k] for k, v in summary_metrics.items() if any(selected_metric in k for selected_metric in metrics)}\n",
        "\n",
        "        data.append(\n",
        "            go.Bar(\n",
        "                x=list(summary_metrics.keys()),\n",
        "                y=list(summary_metrics.values()),\n",
        "                name=title,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    fig = go.Figure(data=data)\n",
        "\n",
        "    # Change the bar mode\n",
        "    fig.update_layout(barmode=\"group\", showlegend=True)\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "def generate_uuid(length: int = 8) -> str:\n",
        "    \"\"\"Generate a uuid of a specified length (default=8).\"\"\"\n",
        "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################### From here, you can follow, once you have loaded the model ###################"
      ],
      "metadata": {
        "id": "Yh77SRdtR7lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load an evaluation dataset"
      ],
      "metadata": {
        "id": "UwUtW2KSaKkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "\n",
        "# Configure your GCS bucket and file\n",
        "bucket_name = \"\"  # Replace with your GCS bucket name\n",
        "file_path = \"\"  # Replace with your file's path in the bucket\n",
        "\n",
        "# Download file from GCS\n",
        "client = storage.Client()\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "blob = bucket.blob(file_path)\n",
        "blob.download_to_filename(\"\")  # Save locally\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"\")\n"
      ],
      "metadata": {
        "id": "czjEA4bNAfai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data check\n",
        "data.head()"
      ],
      "metadata": {
        "id": "28B-30CNBQ1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai import init\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize Vertex AI\n",
        "PROJECT_ID = \"\"  # Replace with your Google Cloud project ID\n",
        "LOCATION = \"us-central1\"  # Adjust based on your model location (e.g., \"europe-west4\")\n",
        "\n",
        "init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "#Print available methods\n",
        "print(dir(model))"
      ],
      "metadata": {
        "id": "G9SKxWP0JcEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noHGa6rD6-ks"
      },
      "source": [
        "### Define a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol1fa0hI7Irr"
      },
      "outputs": [],
      "source": [
        "# Model to be evaluated\n",
        "model = GenerativeModel(\n",
        "    \"gemini-1.5-pro\",\n",
        "    generation_config={\"temperature\": 0.6, \"max_output_tokens\": 50, \"top_k\": 1},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Classification using PHQ-4** ###"
      ],
      "metadata": {
        "id": "QRsYYaJWPPVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gemini Pro 1.5, zero-shot, temp 0.6, BASE (without persona)** ###"
      ],
      "metadata": {
        "id": "g7CmIYSpvWxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "import requests\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "# Define the prompt components first\n",
        "system_instruction = \"none\"\n",
        "criteria = (\n",
        "    \"little interest (little interest or pleasure in doing things), \"\n",
        "    \"hopeless (feeling down, depressed, or hopeless), \"\n",
        "    \"nervous (feeling nervous, anxious or on edge), \"\n",
        "    \"worrying (not being able to stop or control worrying)\"\n",
        ")\n",
        "question = \"What is the likelihood of little interest, hopeless, nervous, and worrying for each message?\"\n",
        "requirements = (\n",
        "    \"Please provide the likelihood for each criterion on a 4-point Likert scale \"\n",
        "    \"(0=not likely, 1=unlikely, 2=likely, 3=very likely) and sum all the Likert scales for each message.\"\n",
        ")\n",
        "\n",
        "def classify_text_with_gemini(text) -> str:\n",
        "    full_prompt = (\n",
        "        f\"{system_instruction}\\n\"\n",
        "        f\"Answer this question: {question} using {criteria} \"\n",
        "        f\"and follow the requirements: {requirements}\\n\"\n",
        "        f\"Respond in the following format strictly, without adding any extra text:\\n\"\n",
        "        f\"1. Little interest: <0-3>\\n\"\n",
        "        f\"2. Hopeless: <0-3>\\n\"\n",
        "        f\"3. Nervous: <0-3>\\n\"\n",
        "        f\"4. Worrying: <0-3>\\n\"\n",
        "        f\"Sum of Likert scales: <sum>\\n\"\n",
        "        f\"Message: {text}\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(full_prompt)\n",
        "\n",
        "        # Safety check: assuming response has 'candidates' and 'finish_reason'\n",
        "        if not response.candidates or response.candidates[0].finish_reason == 'SAFETY':\n",
        "            print(f\"Response blocked by safety filters for text: {text}\")\n",
        "            return None  # Returning None to indicate a blocked response\n",
        "\n",
        "        # Extract the generated text from the response object\n",
        "        # Adjust the attribute access based on your actual response structure\n",
        "        generated_text = response.candidates[0].text  # Example attribute\n",
        "        return generated_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling the model: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_ratings(response: str) -> Tuple[int, Dict[str, int]]:\n",
        "    rating_dict = {\n",
        "        \"0\": 0,\n",
        "        \"1\": 1,\n",
        "        \"2\": 2,\n",
        "        \"3\": 3\n",
        "    }\n",
        "\n",
        "    patterns = {\n",
        "        'little_interest': re.compile(r'little interest\\s*:\\s*(\\d)', re.IGNORECASE),\n",
        "        'hopeless': re.compile(r'hopeless\\s*:\\s*(\\d)', re.IGNORECASE),\n",
        "        'nervous': re.compile(r'nervous\\s*:\\s*(\\d)', re.IGNORECASE),\n",
        "        'worrying': re.compile(r'worrying\\s*:\\s*(\\d)', re.IGNORECASE)\n",
        "    }\n",
        "\n",
        "    scores = {}\n",
        "    total_score = 0\n",
        "\n",
        "    for key, pattern in patterns.items():\n",
        "        match = pattern.search(response)\n",
        "        if match:\n",
        "            rating = match.group(1).strip()\n",
        "            if rating in rating_dict:\n",
        "                score = rating_dict[rating]\n",
        "                scores[key] = score\n",
        "                total_score += score\n",
        "                print(f\"Matched {key}: {rating} -> {score}\")  # Debugging\n",
        "            else:\n",
        "                scores[key] = None\n",
        "                print(f\"No valid rating for {key}: found {rating}, not in rating_dict\")  # Debugging\n",
        "        else:\n",
        "            scores[key] = None\n",
        "            print(f\"No match found for {key} in the response\")  # Debugging\n",
        "\n",
        "    print(f\"Total Score: {total_score}, Scores: {scores}\")  # Additional Debugging\n",
        "    return total_score, scores\n",
        "\n",
        "\n",
        "sum_ratings = []\n",
        "individual_ratings = {'little_interest': [], 'hopeless': [], 'nervous': [], 'worrying': []}\n",
        "responses = []\n",
        "\n",
        "for idx, msg in data['msg'].items():\n",
        "    response = classify_text_with_gemini(msg)\n",
        "    print(f\"Text: {msg}\\nResponse: {response}\\n\")  # Debugging\n",
        "\n",
        "    if response:\n",
        "        total_score, scores = extract_ratings(response)\n",
        "    else:\n",
        "        total_score = None\n",
        "        scores = {'little_interest': None, 'hopeless': None, 'nervous': None, 'worrying': None}\n",
        "        print(\"Empty response received from the model.\")  # Debugging\n",
        "\n",
        "    sum_ratings.append(total_score)\n",
        "    for key in individual_ratings.keys():\n",
        "        individual_ratings[key].append(scores[key])\n",
        "    responses.append(response)\n",
        "    time.sleep(1)\n",
        "\n",
        "# Append the ratings and full responses to your dataframe\n",
        "data['rating'] = sum_ratings\n",
        "for key, values in individual_ratings.items():\n",
        "    data[key] = values\n",
        "data['model_response'] = responses\n",
        "\n",
        "# Save the result\n",
        "data.to_csv(\"\", index=False)\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Be2lMC9jNzFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_data = pd.read_csv (\"\")"
      ],
      "metadata": {
        "id": "z8JqwmWoNzCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_data.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0NWDgDyRNy_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "def upload_to_bucket(bucket_name, source_file_name, destination_blob_name):\n",
        "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
        "\n",
        "    # Initialize a storage client\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    # Get the bucket\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    # Create a blob object from the bucket\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "    # Upload the file\n",
        "    blob.upload_from_filename(source_file_name)\n",
        "\n",
        "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n",
        "\n",
        "\n",
        "# Define your bucket name\n",
        "bucket_name = \"\"\n",
        "\n",
        "# File paths\n",
        "source_file_name = \"\"\n",
        "destination_blob_name = \"\"\n",
        "\n",
        "# Upload the file\n",
        "upload_to_bucket(bucket_name, source_file_name, destination_blob_name)"
      ],
      "metadata": {
        "id": "kZv7RYurNy9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### classification"
      ],
      "metadata": {
        "id": "C01Dm2DluR6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data from the CSV file\n",
        "df = pd.read_csv(\"\")\n",
        "\n",
        "# Add a new 'classification' column based on the 'rating' column\n",
        "df['classification'] = df['rating'].apply(lambda x: 1 if x >= 6 else 0)\n",
        "\n",
        "# Save the updated dataframe back to a CSV file\n",
        "df.to_csv(\"\", index=False)\n",
        "\n",
        "# Print the first few rows to check\n",
        "print(df.head())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-cw9bmf8sQqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# performance with 95% CI"
      ],
      "metadata": {
        "id": "pdW8TNZseOyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "# Load the data from the CSV file (assuming the file with the 'classification' column was saved earlier)\n",
        "df = pd.read_csv(\"\")\n",
        "\n",
        "# Extract the 'label' and 'classification' columns\n",
        "y_true = df['label']\n",
        "y_pred = df['classification']\n",
        "\n",
        "\n",
        "def bootstrap_confidence_interval(y_true, y_pred, metric_func, n_bootstraps=1000, ci=95, **kwargs):\n",
        "    \"\"\"\n",
        "    Calculates the confidence interval for a given metric using bootstrapping.\n",
        "\n",
        "    Parameters:\n",
        "        y_true (pd.Series): True labels.\n",
        "        y_pred (pd.Series): Predicted labels.\n",
        "        metric_func (function): Scikit-learn metric function to calculate (e.g., f1_score).\n",
        "        n_bootstraps (int): Number of bootstrap samples.\n",
        "        ci (float): Confidence level (e.g., 95 for 95% CI).\n",
        "        **kwargs: Additional keyword arguments for the metric function.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Lower and upper bounds of the confidence interval.\n",
        "    \"\"\"\n",
        "    boot_scores = []\n",
        "    n = len(y_true)\n",
        "\n",
        "    for _ in range(n_bootstraps):\n",
        "        # Sample with replacement\n",
        "        indices = np.random.randint(0, n, n)\n",
        "        y_true_boot = y_true.iloc[indices]\n",
        "        y_pred_boot = y_pred.iloc[indices]\n",
        "\n",
        "        # Handle cases where metric might fail (e.g., no positive predictions)\n",
        "        try:\n",
        "            score = metric_func(y_true_boot, y_pred_boot, **kwargs)\n",
        "            boot_scores.append(score)\n",
        "        except ValueError:\n",
        "            continue  # Skip this bootstrap sample if metric calculation fails\n",
        "\n",
        "    # Calculate percentiles for the confidence interval\n",
        "    lower = np.percentile(boot_scores, (100 - ci) / 2)\n",
        "    upper = np.percentile(boot_scores, 100 - (100 - ci) / 2)\n",
        "    return lower, upper\n",
        "\n",
        "# Calculate the point estimates for the metrics\n",
        "f1 = f1_score(y_true, y_pred, average='binary')\n",
        "precision = precision_score(y_true, y_pred, average='binary')\n",
        "recall = recall_score(y_true, y_pred, average='binary')\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "# Calculate the 95% confidence intervals using bootstrapping\n",
        "f1_ci = bootstrap_confidence_interval(y_true, y_pred, f1_score, average='binary')\n",
        "precision_ci = bootstrap_confidence_interval(y_true, y_pred, precision_score, average='binary')\n",
        "recall_ci = bootstrap_confidence_interval(y_true, y_pred, recall_score, average='binary')\n",
        "accuracy_ci = bootstrap_confidence_interval(y_true, y_pred, accuracy_score)\n",
        "\n",
        "# Print the results\n",
        "print(f'F1 Score: {f1:.4f} (95% CI: {f1_ci[0]:.4f} - {f1_ci[1]:.4f})')\n",
        "print(f'Precision: {precision:.4f} (95% CI: {precision_ci[0]:.4f} - {precision_ci[1]:.4f})')\n",
        "print(f'Recall: {recall:.4f} (95% CI: {recall_ci[0]:.4f} - {recall_ci[1]:.4f})')\n",
        "print(f'Accuracy: {accuracy:.4f} (95% CI: {accuracy_ci[0]:.4f} - {accuracy_ci[1]:.4f})')"
      ],
      "metadata": {
        "id": "5p9mFN-BNy6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-C7znZmkNy3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oZILgH92Ny0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gemini Pro 1.5, temp 0.6, zero-shot, systemic persona**"
      ],
      "metadata": {
        "id": "D87Cn3yBv9q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model to be evaluated\n",
        "model = GenerativeModel(\n",
        "    \"gemini-1.5-pro\",\n",
        "    generation_config={\"temperature\": 0.6, \"max_output_tokens\": 50, \"top_k\": 1},\n",
        ")"
      ],
      "metadata": {
        "id": "UFionUNev8FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the dataset\n",
        "\n",
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "\n",
        "# Configure your GCS bucket and file\n",
        "bucket_name = \"\"  # Replace with your GCS bucket name\n",
        "file_path = \"\"  # Replace with your file's path in the bucket\n",
        "\n",
        "# Download file from GCS\n",
        "client = storage.Client()\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "blob = bucket.blob(file_path)\n",
        "blob.download_to_filename(\"\")  # Save locally\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"\")\n"
      ],
      "metadata": {
        "id": "7k4Rea0cfm3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "import requests\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "# Define the prompt components first\n",
        "system_instruction = (\n",
        "    \"As Dr. Gemini, a professional psychiatrist in the hospital,\"\n",
        "    \"one of your roles is responding to the patients messages through the patient portal.\"\n",
        "    \"Patients ask health questions and report some symptoms or lab results.\"\n",
        "    \"I will ask for you to evaluate the patients messages to screen those who would need further assessment for depression or anxiety.\"\n",
        "    \"I myself am a psychiatrist in the hospital. I will use your evaluation to better detect\"\n",
        "    \"patients depressive and anxious symptoms timely through the patient portal messages.\"\n",
        "    \"For each question, you should take time to think deeply and step-by-step to be sure to offer\"\n",
        "    \"the right answer in a professional manner. To begin, please confirm that you understand your role.\"\n",
        ")\n",
        "\n",
        "criteria = (\n",
        "    \"little interest (little interest or pleasure in doing things), \"\n",
        "    \"hopeless (feeling down, depressed, or hopeless), \"\n",
        "    \"nervous (feeling nervous, anxious or on edge), \"\n",
        "    \"worrying (not being able to stop or control worrying)\"\n",
        ")\n",
        "question = \"What is the likelihood of little interest, hopeless, nervous, and worrying for each message?\"\n",
        "requirements = (\n",
        "    \"Please provide the likelihood for each criterion on a 4-point Likert scale \"\n",
        "    \"(0=not likely, 1=unlikely, 2=likely, 3=very likely) and sum all the Likert scales for each message.\"\n",
        ")\n",
        "\n",
        "def classify_text_with_gemini(text) -> str:\n",
        "    full_prompt = (\n",
        "        f\"{system_instruction}\\n\"\n",
        "        f\"Answer this question: {question} using {criteria} \"\n",
        "        f\"and follow the requirements: {requirements}\\n\"\n",
        "        f\"Respond in the following format strictly, without adding any extra text:\\n\"\n",
        "        f\"1. Little interest: <0-3>\\n\"\n",
        "        f\"2. Hopeless: <0-3>\\n\"\n",
        "        f\"3. Nervous: <0-3>\\n\"\n",
        "        f\"4. Worrying: <0-3>\\n\"\n",
        "        f\"Sum of Likert scales: <sum>\\n\"\n",
        "        f\"Message: {text}\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(full_prompt)\n",
        "\n",
        "        # Safety check: assuming response has 'candidates' and 'finish_reason'\n",
        "        if not response.candidates or response.candidates[0].finish_reason == 'SAFETY':\n",
        "            print(f\"Response blocked by safety filters for text: {text}\")\n",
        "            return None  # Returning None to indicate a blocked response\n",
        "\n",
        "        # Extract the generated text from the response object\n",
        "        # Adjust the attribute access based on your actual response structure\n",
        "        generated_text = response.candidates[0].text  # Example attribute\n",
        "        return generated_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling the model: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_ratings(response: str) -> Tuple[int, Dict[str, int]]:\n",
        "    rating_dict = {\n",
        "        \"0\": 0,\n",
        "        \"1\": 1,\n",
        "        \"2\": 2,\n",
        "        \"3\": 3\n",
        "    }\n",
        "\n",
        "    patterns = {\n",
        "        'little_interest': re.compile(r'little interest\\s*:\\s*(\\d)', re.IGNORECASE),\n",
        "        'hopeless': re.compile(r'hopeless\\s*:\\s*(\\d)', re.IGNORECASE),\n",
        "        'nervous': re.compile(r'nervous\\s*:\\s*(\\d)', re.IGNORECASE),\n",
        "        'worrying': re.compile(r'worrying\\s*:\\s*(\\d)', re.IGNORECASE)\n",
        "    }\n",
        "\n",
        "    scores = {}\n",
        "    total_score = 0\n",
        "\n",
        "    for key, pattern in patterns.items():\n",
        "        match = pattern.search(response)\n",
        "        if match:\n",
        "            rating = match.group(1).strip()\n",
        "            if rating in rating_dict:\n",
        "                score = rating_dict[rating]\n",
        "                scores[key] = score\n",
        "                total_score += score\n",
        "                print(f\"Matched {key}: {rating} -> {score}\")  # Debugging\n",
        "            else:\n",
        "                scores[key] = None\n",
        "                print(f\"No valid rating for {key}: found {rating}, not in rating_dict\")  # Debugging\n",
        "        else:\n",
        "            scores[key] = None\n",
        "            print(f\"No match found for {key} in the response\")  # Debugging\n",
        "\n",
        "    print(f\"Total Score: {total_score}, Scores: {scores}\")  # Additional Debugging\n",
        "    return total_score, scores\n",
        "\n",
        "\n",
        "sum_ratings = []\n",
        "individual_ratings = {'little_interest': [], 'hopeless': [], 'nervous': [], 'worrying': []}\n",
        "responses = []\n",
        "\n",
        "for idx, msg in data['msg'].items():\n",
        "    response = classify_text_with_gemini(msg)\n",
        "    print(f\"Text: {msg}\\nResponse: {response}\\n\")  # Debugging\n",
        "\n",
        "    if response:\n",
        "        total_score, scores = extract_ratings(response)\n",
        "    else:\n",
        "        total_score = None\n",
        "        scores = {'little_interest': None, 'hopeless': None, 'nervous': None, 'worrying': None}\n",
        "        print(\"Empty response received from the model.\")  # Debugging\n",
        "\n",
        "    sum_ratings.append(total_score)\n",
        "    for key in individual_ratings.keys():\n",
        "        individual_ratings[key].append(scores[key])\n",
        "    responses.append(response)\n",
        "    time.sleep(1)\n",
        "\n",
        "# Append the ratings and full responses to dataframe\n",
        "data['rating'] = sum_ratings\n",
        "for key, values in individual_ratings.items():\n",
        "    data[key] = values\n",
        "data['model_response'] = responses\n",
        "\n",
        "# Save the result\n",
        "data.to_csv(\"\", index=False)\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "id": "OoSRCJvKwI8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_data = pd.read_csv(\"\")"
      ],
      "metadata": {
        "id": "AqUrw_m4wI5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data check\n",
        "saved_data.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bd2L1ZIUwI2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "def upload_to_bucket(bucket_name, source_file_name, destination_blob_name):\n",
        "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
        "\n",
        "    # Initialize a storage client\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    # Get the bucket\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    # Create a blob object from the bucket\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "    # Upload the file\n",
        "    blob.upload_from_filename(source_file_name)\n",
        "\n",
        "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n",
        "\n",
        "\n",
        "# Define your bucket name\n",
        "bucket_name = \"\"\n",
        "\n",
        "# File paths\n",
        "source_file_name = \"\"\n",
        "destination_blob_name = \"\"  # Change the path if needed\n",
        "\n",
        "# Upload the file\n",
        "upload_to_bucket(bucket_name, source_file_name, destination_blob_name)"
      ],
      "metadata": {
        "id": "broVUNTGNMQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data from the CSV file\n",
        "df = pd.read_csv(\"\")\n",
        "\n",
        "# Add a new 'classification' column based on the 'rating' column\n",
        "df['classification'] = df['rating'].apply(lambda x: 1 if x >= 6 else 0)\n",
        "\n",
        "# Save the updated dataframe back to a CSV file\n",
        "df.to_csv(\"\", index=False)\n",
        "\n",
        "# Print the first few rows to check\n",
        "print(df.head())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nDZOO4jcNMJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# performance WITH 95% CI"
      ],
      "metadata": {
        "id": "HhpfwFpuv7_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "# Load the data from the CSV file (assuming the file with the 'classification' column was saved earlier)\n",
        "df = pd.read_csv(\"\")\n",
        "\n",
        "# Extract the 'label' and 'classification' columns\n",
        "y_true = df['label']\n",
        "y_pred = df['classification']\n",
        "\n",
        "\n",
        "def bootstrap_confidence_interval(y_true, y_pred, metric_func, n_bootstraps=1000, ci=95, **kwargs):\n",
        "    \"\"\"\n",
        "    Calculates the confidence interval for a given metric using bootstrapping.\n",
        "\n",
        "    Parameters:\n",
        "        y_true (pd.Series): True labels.\n",
        "        y_pred (pd.Series): Predicted labels.\n",
        "        metric_func (function): Scikit-learn metric function to calculate (e.g., f1_score).\n",
        "        n_bootstraps (int): Number of bootstrap samples.\n",
        "        ci (float): Confidence level (e.g., 95 for 95% CI).\n",
        "        **kwargs: Additional keyword arguments for the metric function.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Lower and upper bounds of the confidence interval.\n",
        "    \"\"\"\n",
        "    boot_scores = []\n",
        "    n = len(y_true)\n",
        "\n",
        "    for _ in range(n_bootstraps):\n",
        "        # Sample with replacement\n",
        "        indices = np.random.randint(0, n, n)\n",
        "        y_true_boot = y_true.iloc[indices]\n",
        "        y_pred_boot = y_pred.iloc[indices]\n",
        "\n",
        "        # Handle cases where metric might fail (e.g., no positive predictions)\n",
        "        try:\n",
        "            score = metric_func(y_true_boot, y_pred_boot, **kwargs)\n",
        "            boot_scores.append(score)\n",
        "        except ValueError:\n",
        "            continue  # Skip this bootstrap sample if metric calculation fails\n",
        "\n",
        "    # Calculate percentiles for the confidence interval\n",
        "    lower = np.percentile(boot_scores, (100 - ci) / 2)\n",
        "    upper = np.percentile(boot_scores, 100 - (100 - ci) / 2)\n",
        "    return lower, upper\n",
        "\n",
        "# Calculate the point estimates for the metrics\n",
        "f1 = f1_score(y_true, y_pred, average='binary')\n",
        "precision = precision_score(y_true, y_pred, average='binary')\n",
        "recall = recall_score(y_true, y_pred, average='binary')\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "# Calculate the 95% confidence intervals using bootstrapping\n",
        "f1_ci = bootstrap_confidence_interval(y_true, y_pred, f1_score, average='binary')\n",
        "precision_ci = bootstrap_confidence_interval(y_true, y_pred, precision_score, average='binary')\n",
        "recall_ci = bootstrap_confidence_interval(y_true, y_pred, recall_score, average='binary')\n",
        "accuracy_ci = bootstrap_confidence_interval(y_true, y_pred, accuracy_score)\n",
        "\n",
        "# Print the results\n",
        "print(f'F1 Score: {f1:.4f} (95% CI: {f1_ci[0]:.4f} - {f1_ci[1]:.4f})')\n",
        "print(f'Precision: {precision:.4f} (95% CI: {precision_ci[0]:.4f} - {precision_ci[1]:.4f})')\n",
        "print(f'Recall: {recall:.4f} (95% CI: {recall_ci[0]:.4f} - {recall_ci[1]:.4f})')\n",
        "print(f'Accuracy: {accuracy:.4f} (95% CI: {accuracy_ci[0]:.4f} - {accuracy_ci[1]:.4f})')"
      ],
      "metadata": {
        "id": "SHBB9pMGv78R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IliOsH-iOKiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FAw6H5GUOKfr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "MH_LLM_Supplementrary_2_Gemini_PHQ4_Persona.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}