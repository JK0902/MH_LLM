{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Vertex AI Model Garden - Llama 3.1 models\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###################### Vertex AI setting up code provided by Vertex AI Model Garden #####################################"
      ],
      "metadata": {
        "id": "7fqt4Szgp6NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK for Python and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade --quiet google-cloud-aiplatform[langchain] openai\n",
        "! pip3 install --upgrade --quiet langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages, you must restart the runtime on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XRvKdaPDTznN"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "Authenticate your environment on Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NyKGtVQjgx13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df04a279-8ce1-4342-dcec-a7c94284ee1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: google.colab.auth.authenticate_user() is not supported in Colab Enterprise.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "LOCATION = \"\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store tutorial artifacts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wn8ZkcV86KR"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B8DawN9D9NLU"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVYoyDl165EE"
      },
      "source": [
        "### Import libraries\n",
        "\n",
        "Import libraries to use in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "c1tEW-U968h8"
      },
      "outputs": [],
      "source": [
        "# Chat completions API\n",
        "import openai\n",
        "from google.auth import default, transport\n",
        "from langchain import PromptTemplate\n",
        "# Build\n",
        "from langchain_openai import ChatOpenAI\n",
        "from vertexai.preview import rag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqYCG2Fw7D3L"
      },
      "source": [
        "### Configure OpenAI SDK for the Llama 3.1 Chat Completions API\n",
        "\n",
        "To configure the OpenAI SDK for the Llama 3.1 Chat Completions API, you need to request the access token and initialize the client pointing to the Llama 3.1 endpoint.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0K6VSJRHhH2"
      },
      "source": [
        "#### Authentication\n",
        "\n",
        "You can request an access token from the default credentials for the current environment. Note that the access token lives for [1 hour by default](https://cloud.google.com/docs/authentication/token-types#at-lifetime); after expiration, it must be refreshed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "i0qceuiQEPHv"
      },
      "outputs": [],
      "source": [
        "credentials, _ = default()\n",
        "auth_request = transport.requests.Request()\n",
        "credentials.refresh(auth_request)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q04wJmA0HT6X"
      },
      "source": [
        "Then configure the OpenAI SDK to point to the Llama 3.1 Chat Completions API endpoint.\n",
        "\n",
        "Notice, only `us-central1` is supported region for Llama 3.1 models using Model-as-a-Service (MaaS)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "c-MRhsnlj6iw"
      },
      "outputs": [],
      "source": [
        "MODEL_LOCATION = \"us-central1\"\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    base_url=f\"https://{MODEL_LOCATION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{MODEL_LOCATION}/endpoints/openapi/chat/completions?\",\n",
        "    api_key=credentials.token,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7OhyH46H2H5"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"meta/llama-3.1-8b-instruct-maas\"  # @param {type:\"string\"} [\"meta/llama-3.1-8b-instruct-maas\", \"meta/llama-3.1-70b-instruct-maas\", \"meta/llama-3.1-405b-instruct-maas\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1rKbHUQt605"
      },
      "source": [
        "#### Ask Llama 3.1 using different model configuration\n",
        "\n",
        "Use the following parameters to generate different answers:\n",
        "\n",
        "*   `temperature` to control the randomness of the response\n",
        "*   `max_tokens` to limit the response length\n",
        "*   `top_p` to control the quality of the response\n",
        "*   `stream` to stream the response back or not\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owv-5Sz5rIEU"
      },
      "outputs": [],
      "source": [
        "temperature = 0.6  # @param {type:\"number\"}\n",
        "max_tokens = 50  # @param {type:\"integer\"}\n",
        "top_p = 1.0  # @param {type:\"number\"}\n",
        "stream = True  # @param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load your model - We used Vertex AI Model Garden**"
      ],
      "metadata": {
        "id": "l485ZryKrC9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############################ Llama 3.1 8B model loaded #######################################"
      ],
      "metadata": {
        "id": "q9SGtAXyrQZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Llama 3.1_8B, temp 0.6, Base (without persona), zero-shot**"
      ],
      "metadata": {
        "id": "3CMt2OOzql9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "\n",
        "# Configure your GCS bucket and file\n",
        "bucket_name = \"\"  # Replace with your GCS bucket name\n",
        "file_path = \"\"  # Replace with your file's path in the bucket\n",
        "\n",
        "# Download file from GCS\n",
        "client = storage.Client()\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "blob = bucket.blob(file_path)\n",
        "blob.download_to_filename(\"\")  # Save locally\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"\")"
      ],
      "metadata": {
        "id": "EeXvH0uDPu3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "# Define a function to call the Llama model\n",
        "def llama_model_fn(text: str) -> Tuple[int, str]:\n",
        "    try:\n",
        "        system_instruction = \"none\"\n",
        "        full_prompt = (\n",
        "            f\"{system_instruction}\\n\"\n",
        "            f\"Classify the following message as 1 if the person seems to have depression or anxiety, otherwise 0, and provide the reasoning.\\n\"\n",
        "            f\"Output format:\\n\"\n",
        "            f\"Classification: <0 or 1>\\n\"\n",
        "            f\"Reason: <reason_text>\\n\\n\"\n",
        "            f\"Message:\\n{text}\"\n",
        "        )\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=MODEL_ID,\n",
        "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "        )\n",
        "\n",
        "        # Corrected line: Use dot notation to access 'content'\n",
        "        content = response.choices[0].message.content.strip()\n",
        "        print(f\"Extracted content: {content}\")  # Debug: Print extracted content\n",
        "\n",
        "        # Initialize default values\n",
        "        classification = 0\n",
        "        reasoning = \"No reason provided.\"\n",
        "\n",
        "        # Parse the response\n",
        "        parts = content.split('\\n')\n",
        "        for part in parts:\n",
        "            if \"Classification:\" in part:\n",
        "                try:\n",
        "                    classification = int(part.split(\":\", 1)[1].strip())\n",
        "                except ValueError:\n",
        "                    classification = 0  # Default to 0 in case of error\n",
        "            elif \"Reason:\" in part:\n",
        "                reasoning = part.split(\":\", 1)[1].strip()\n",
        "\n",
        "        print(f\"Parsed values -> Classification: {classification}, Reason: {reasoning}\")\n",
        "        return classification, reasoning\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing message: {e}\")\n",
        "        return 0, \"Error processing message.\"\n",
        "\n",
        "# Initialize lists to store classifications and reasons\n",
        "classifications = []\n",
        "reasons = []\n",
        "\n",
        "# Process each entry in the dataset\n",
        "for index, row in data.iterrows():\n",
        "    message = row.get(\"msg\", \"\")  # Safely get 'msg' column\n",
        "    if not message:\n",
        "        print(f\"Row {index} has no message. Skipping.\")\n",
        "        classifications.append(0)\n",
        "        reasons.append(\"No message provided.\")\n",
        "        continue\n",
        "\n",
        "    classification, reasoning = llama_model_fn(message)\n",
        "    classifications.append(classification)\n",
        "    reasons.append(reasoning)\n",
        "    time.sleep(1)  # Sleep for 1 second to avoid too many requests\n",
        "\n",
        "    # Optional: Print progress\n",
        "    if (index + 1) % 10 == 0 or (index + 1) == len(data):\n",
        "        print(f\"Processed {index + 1}/{len(data)} messages.\")\n",
        "\n",
        "# Add classifications and reasons to the dataframe\n",
        "data[\"predictions\"] = classifications\n",
        "data[\"reasoning\"] = reasons\n",
        "\n",
        "# Handle cases where classification was None or NaN\n",
        "data[\"predictions\"].fillna(0, inplace=True)\n",
        "\n",
        "# Save the results to a CSV for review\n",
        "output_file = \"\"\n",
        "data.to_csv(output_file, index=False)\n",
        "print(f\"Classifications saved to '{output_file}'.\")\n",
        "print(data)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "69s3gY6vVzJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_data=pd.read_csv(\"\")"
      ],
      "metadata": {
        "id": "Wg8Mt5zc86oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_data.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gtiZV9sp86lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "def upload_to_bucket(bucket_name, source_file_name, destination_blob_name):\n",
        "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
        "\n",
        "    # Initialize a storage client\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    # Get the bucket\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    # Create a blob object from the bucket\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "    # Upload the file\n",
        "    blob.upload_from_filename(source_file_name)\n",
        "\n",
        "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n",
        "\n",
        "\n",
        "# Define your bucket name\n",
        "bucket_name = \"\"\n",
        "\n",
        "# File paths\n",
        "source_file_name = \"\"\n",
        "destination_blob_name =  \"\"\n",
        "\n",
        "# Upload the file\n",
        "upload_to_bucket(bucket_name, source_file_name, destination_blob_name)"
      ],
      "metadata": {
        "id": "KqY6_42zO_8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the data for classification"
      ],
      "metadata": {
        "id": "YIt7acn-UAo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "\n",
        "# Configure your GCS bucket and file\n",
        "bucket_name = \"\"  # Replace with your GCS bucket name\n",
        "file_path = \"\"  # Replace with your file's path in the bucket\n",
        "\n",
        "# Download file from GCS\n",
        "client = storage.Client()\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "blob = bucket.blob(file_path)\n",
        "blob.download_to_filename(\"\")  # Save locally\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"\")"
      ],
      "metadata": {
        "id": "XAjSanRHUAl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification performance with 95% CI"
      ],
      "metadata": {
        "id": "d11Vnkv7EdOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "# Load the data from the CSV file (assuming the file with the 'classification' column was saved earlier)\n",
        "df = pd.read_csv(\"\")\n",
        "\n",
        "# Extract the 'label' and 'classification' columns\n",
        "y_true = df['label']\n",
        "y_pred = df['predictions']\n",
        "\n",
        "def bootstrap_confidence_interval(y_true, y_pred, metric_func, n_bootstraps=1000, ci=95, **kwargs):\n",
        "    \"\"\"\n",
        "    Calculates the confidence interval for a given metric using bootstrapping.\n",
        "\n",
        "    Parameters:\n",
        "        y_true (pd.Series): True labels.\n",
        "        y_pred (pd.Series): Predicted labels.\n",
        "        metric_func (function): Scikit-learn metric function to calculate (e.g., f1_score).\n",
        "        n_bootstraps (int): Number of bootstrap samples.\n",
        "        ci (float): Confidence level (e.g., 95 for 95% CI).\n",
        "        **kwargs: Additional keyword arguments for the metric function.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Lower and upper bounds of the confidence interval.\n",
        "    \"\"\"\n",
        "    boot_scores = []\n",
        "    n = len(y_true)\n",
        "\n",
        "    for _ in range(n_bootstraps):\n",
        "        # Sample with replacement\n",
        "        indices = np.random.randint(0, n, n)\n",
        "        y_true_boot = y_true.iloc[indices]\n",
        "        y_pred_boot = y_pred.iloc[indices]\n",
        "\n",
        "        # Handle cases where metric might fail (e.g., no positive predictions)\n",
        "        try:\n",
        "            score = metric_func(y_true_boot, y_pred_boot, **kwargs)\n",
        "            boot_scores.append(score)\n",
        "        except ValueError:\n",
        "            continue  # Skip this bootstrap sample if metric calculation fails\n",
        "\n",
        "    # Calculate percentiles for the confidence interval\n",
        "    lower = np.percentile(boot_scores, (100 - ci) / 2)\n",
        "    upper = np.percentile(boot_scores, 100 - (100 - ci) / 2)\n",
        "    return lower, upper\n",
        "\n",
        "# Calculate the point estimates for the metrics\n",
        "f1 = f1_score(y_true, y_pred, average='binary')\n",
        "precision = precision_score(y_true, y_pred, average='binary')\n",
        "recall = recall_score(y_true, y_pred, average='binary')\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "# Calculate the 95% confidence intervals using bootstrapping\n",
        "f1_ci = bootstrap_confidence_interval(y_true, y_pred, f1_score, average='binary')\n",
        "precision_ci = bootstrap_confidence_interval(y_true, y_pred, precision_score, average='binary')\n",
        "recall_ci = bootstrap_confidence_interval(y_true, y_pred, recall_score, average='binary')\n",
        "accuracy_ci = bootstrap_confidence_interval(y_true, y_pred, accuracy_score)\n",
        "\n",
        "# Print the results\n",
        "print(f'F1 Score: {f1:.4f} (95% CI: {f1_ci[0]:.4f} - {f1_ci[1]:.4f})')\n",
        "print(f'Precision: {precision:.4f} (95% CI: {precision_ci[0]:.4f} - {precision_ci[1]:.4f})')\n",
        "print(f'Recall: {recall:.4f} (95% CI: {recall_ci[0]:.4f} - {recall_ci[1]:.4f})')\n",
        "print(f'Accuracy: {accuracy:.4f} (95% CI: {accuracy_ci[0]:.4f} - {accuracy_ci[1]:.4f})')\n"
      ],
      "metadata": {
        "id": "YkOzq9yhVQfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ok36gSLBVQcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Llama 3.1_8B, temp 0.6, Base (without persona), few-shots (n=2)**"
      ],
      "metadata": {
        "id": "dYu2PS0AAadZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "\n",
        "# Configure your GCS bucket and file\n",
        "bucket_name = \"\"  # Replace with your GCS bucket name\n",
        "file_path = \"\"  # Replace with your file's path in the bucket\n",
        "\n",
        "# Download file from GCS\n",
        "client = storage.Client()\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "blob = bucket.blob(file_path)\n",
        "blob.download_to_filename(\"\")  # Save locally\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"\")"
      ],
      "metadata": {
        "id": "J1GJCGyGq6Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "# Define a function to call the Llama model\n",
        "def llama_model_fn(text: str) -> Tuple[int, str]:\n",
        "    try:\n",
        "        system_instruction = \"none\"\n",
        "        reference = (\n",
        "    \"Example 1:\\n\"\n",
        "\n",
        "    \"Example 2:\\n\"\n",
        "\n",
        ")\n",
        "\n",
        "        full_prompt = (\n",
        "            f\"{system_instruction}\\n\"\n",
        "            f\"Learn from: {reference}\\n\"\n",
        "            f\"Classify the following message as 1 if the person seems to have depression or anxiety, otherwise 0, and provide the reasoning.\\n\"\n",
        "            f\"Output format:\\n\"\n",
        "            f\"Classification: <0 or 1>\\n\"\n",
        "            f\"Reason: <reason_text>\\n\\n\"\n",
        "            f\"Message:\\n{text}\"\n",
        "        )\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=MODEL_ID,\n",
        "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "        )\n",
        "\n",
        "        # Corrected line: Use dot notation to access 'content'\n",
        "        content = response.choices[0].message.content.strip()\n",
        "        print(f\"Extracted content: {content}\")  # Debug: Print extracted content\n",
        "\n",
        "        # Initialize default values\n",
        "        classification = 0\n",
        "        reasoning = \"No reason provided.\"\n",
        "\n",
        "        # Parse the response\n",
        "        parts = content.split('\\n')\n",
        "        for part in parts:\n",
        "            if \"Classification:\" in part:\n",
        "                try:\n",
        "                    classification = int(part.split(\":\", 1)[1].strip())\n",
        "                except ValueError:\n",
        "                    classification = 0  # Default to 0 in case of error\n",
        "            elif \"Reason:\" in part:\n",
        "                reasoning = part.split(\":\", 1)[1].strip()\n",
        "\n",
        "        print(f\"Parsed values -> Classification: {classification}, Reason: {reasoning}\")\n",
        "        return classification, reasoning\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing message: {e}\")\n",
        "        return 0, \"Error processing message.\"\n",
        "\n",
        "# Initialize lists to store classifications and reasons\n",
        "classifications = []\n",
        "reasons = []\n",
        "\n",
        "# Process each entry in the dataset\n",
        "for index, row in data.iterrows():\n",
        "    message = row.get(\"msg\", \"\")  # Safely get 'msg' column\n",
        "    if not message:\n",
        "        print(f\"Row {index} has no message. Skipping.\")\n",
        "        classifications.append(0)\n",
        "        reasons.append(\"No message provided.\")\n",
        "        continue\n",
        "\n",
        "    classification, reasoning = llama_model_fn(message)\n",
        "    classifications.append(classification)\n",
        "    reasons.append(reasoning)\n",
        "    time.sleep(1)  # Sleep for 1 second to avoid too many requests\n",
        "\n",
        "    # Optional: Print progress\n",
        "    if (index + 1) % 10 == 0 or (index + 1) == len(data):\n",
        "        print(f\"Processed {index + 1}/{len(data)} messages.\")\n",
        "\n",
        "# Add classifications and reasons to the dataframe\n",
        "data[\"predictions\"] = classifications\n",
        "data[\"reasoning\"] = reasons\n",
        "\n",
        "# Handle cases where classification was None or NaN\n",
        "data[\"predictions\"].fillna(0, inplace=True)\n",
        "\n",
        "# Save the results to a CSV for review\n",
        "output_file = \"\"\n",
        "data.to_csv(output_file, index=False)\n",
        "print(f\"Classifications saved to '{output_file}'.\")\n",
        "print(data)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vrqLS7IxeLU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_data = pd.read_csv(\"\")"
      ],
      "metadata": {
        "id": "02_DKu7n_Agm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_data.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Z4WUbNso_Adv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "def upload_to_bucket(bucket_name, source_file_name, destination_blob_name):\n",
        "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
        "\n",
        "    # Initialize a storage client\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    # Get the bucket\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    # Create a blob object from the bucket\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "    # Upload the file\n",
        "    blob.upload_from_filename(source_file_name)\n",
        "\n",
        "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n",
        "\n",
        "\n",
        "# Define your bucket name\n",
        "bucket_name = \"\"\n",
        "\n",
        "# File paths\n",
        "source_file_name = \"\"\n",
        "destination_blob_name = \"\"\n",
        "\n",
        "# Upload the file\n",
        "upload_to_bucket(bucket_name, source_file_name, destination_blob_name)"
      ],
      "metadata": {
        "id": "ALEoVnjj_Aap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the data for classification"
      ],
      "metadata": {
        "id": "Osp5M-h8HBF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "\n",
        "# Configure your GCS bucket and file\n",
        "bucket_name = \"\"  # Replace with your GCS bucket name\n",
        "file_path = \"\"  # Replace with your file's path in the bucket\n",
        "\n",
        "# Download file from GCS\n",
        "client = storage.Client()\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "blob = bucket.blob(file_path)\n",
        "blob.download_to_filename(\"\")  # Save locally\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"\")"
      ],
      "metadata": {
        "id": "F4XK_QlKEQ_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classification performance with 95% CI"
      ],
      "metadata": {
        "id": "iaQHPB7RVw-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "# Load the data from the CSV file (assuming the file with the 'classification' column was saved earlier)\n",
        "df = pd.read_csv(\"\")\n",
        "\n",
        "# Extract the 'label' and 'classification' columns\n",
        "y_true = df['label']\n",
        "y_pred = df['predictions']\n",
        "\n",
        "def bootstrap_confidence_interval(y_true, y_pred, metric_func, n_bootstraps=1000, ci=95, **kwargs):\n",
        "    \"\"\"\n",
        "    Calculates the confidence interval for a given metric using bootstrapping.\n",
        "\n",
        "    Parameters:\n",
        "        y_true (pd.Series): True labels.\n",
        "        y_pred (pd.Series): Predicted labels.\n",
        "        metric_func (function): Scikit-learn metric function to calculate (e.g., f1_score).\n",
        "        n_bootstraps (int): Number of bootstrap samples.\n",
        "        ci (float): Confidence level (e.g., 95 for 95% CI).\n",
        "        **kwargs: Additional keyword arguments for the metric function.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Lower and upper bounds of the confidence interval.\n",
        "    \"\"\"\n",
        "    boot_scores = []\n",
        "    n = len(y_true)\n",
        "\n",
        "    for _ in range(n_bootstraps):\n",
        "        # Sample with replacement\n",
        "        indices = np.random.randint(0, n, n)\n",
        "        y_true_boot = y_true.iloc[indices]\n",
        "        y_pred_boot = y_pred.iloc[indices]\n",
        "\n",
        "        # Handle cases where metric might fail (e.g., no positive predictions)\n",
        "        try:\n",
        "            score = metric_func(y_true_boot, y_pred_boot, **kwargs)\n",
        "            boot_scores.append(score)\n",
        "        except ValueError:\n",
        "            continue  # Skip this bootstrap sample if metric calculation fails\n",
        "\n",
        "    # Calculate percentiles for the confidence interval\n",
        "    lower = np.percentile(boot_scores, (100 - ci) / 2)\n",
        "    upper = np.percentile(boot_scores, 100 - (100 - ci) / 2)\n",
        "    return lower, upper\n",
        "\n",
        "# Calculate the point estimates for the metrics\n",
        "f1 = f1_score(y_true, y_pred, average='binary')\n",
        "precision = precision_score(y_true, y_pred, average='binary')\n",
        "recall = recall_score(y_true, y_pred, average='binary')\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "# Calculate the 95% confidence intervals using bootstrapping\n",
        "f1_ci = bootstrap_confidence_interval(y_true, y_pred, f1_score, average='binary')\n",
        "precision_ci = bootstrap_confidence_interval(y_true, y_pred, precision_score, average='binary')\n",
        "recall_ci = bootstrap_confidence_interval(y_true, y_pred, recall_score, average='binary')\n",
        "accuracy_ci = bootstrap_confidence_interval(y_true, y_pred, accuracy_score)\n",
        "\n",
        "# Print the results\n",
        "print(f'F1 Score: {f1:.4f} (95% CI: {f1_ci[0]:.4f} - {f1_ci[1]:.4f})')\n",
        "print(f'Precision: {precision:.4f} (95% CI: {precision_ci[0]:.4f} - {precision_ci[1]:.4f})')\n",
        "print(f'Recall: {recall:.4f} (95% CI: {recall_ci[0]:.4f} - {recall_ci[1]:.4f})')\n",
        "print(f'Accuracy: {accuracy:.4f} (95% CI: {accuracy_ci[0]:.4f} - {accuracy_ci[1]:.4f})')\n"
      ],
      "metadata": {
        "id": "Gz-hhNzhVw7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ymNqRvjYHBCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IbVHoW_-JsJj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "MH_LLM_Supplementary_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}